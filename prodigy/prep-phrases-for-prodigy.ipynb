{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Input for Prodigy Software\n",
    "\n",
    "This process prepares phrases that can be utilized by the Prodigy Software\n",
    "for annotation purposes.  We are currently exploring the use of prodigy to\n",
    "train specific NER tags using SpaCy.  Contact dwieferich@usgs.gov for additional details.\n",
    "\n",
    "\n",
    "* be in the format\n",
    "\n",
    "```\n",
    "text\n",
    "\n",
    "# or \n",
    "\n",
    "{'text':'klsadjls'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example input data (file_name) produced by running\n",
    "https://github.com/bserna-usgs/app-template/tree/master/app\n",
    "(The referenced app was developed by USGS to run against the GeoDeepDive Database, now being called eXtract Dark Data (xDD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dam mention passages\n",
    "import ast\n",
    "import pandas as pd\n",
    "file_name = 'dam_year_22h43m_07Dec2018_603d272/dam-cand-df.csv'\n",
    "chunksize = 10 ** 6\n",
    "\n",
    "\n",
    "for chunk in pd.read_csv(file_name, chunksize=chunksize):\n",
    "    \n",
    "    with open('dam_samples.txt', 'w', encoding='utf-8') as f:\n",
    "\n",
    "        for index, row in chunk.iterrows():\n",
    "            x = ast.literal_eval(row['passage'])\n",
    "            text = ' '.join(word for word in x)\n",
    "            f.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These age estimates are suitably internally consistent , in that the older OSL ages are estimating the time that late Wisconsin ice was at its maximum extent and outwash was aggrading down Marsh Valley and Swamplovers Valley , whereas the younger radiocarbon dates are limiting the reestablishment of lakes in those valleys after ice had retreated from the headwaters and sufﬁcient outwash had continued to aggrade in the Wisconsin River and Black Earth Creek to dam those tributaries . Furthermore , the OSL age of 21.4 ± 3.3 ka collected from the top of the outwash in Swamplovers Valley , and 23.4 ± 3.7 ka and 26.4 ± 5.1 ka from lacustrine sediment immediately underlying the outwash provide an estimate for when ice was at its maximum extent at the site . This is broadly consistent with the estimate of 18.5 ka from the Baraboo Hills by Attig et al. -LRB- 2011 -RRB- as the time when ice had begun thinning and retreating .\n"
     ]
    }
   ],
   "source": [
    "#Print the last line of text to show an example of final format.\n",
    "#Notice how this example mentions the word dam but not in reference to \n",
    "#a name of a dam, which is what we are interested in.\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = 'dam_year_river_22h33m_06Nov2018_a4c1766/river-cand-df.csv'\n",
    "chunksize = 10 ** 6\n",
    "\n",
    "\n",
    "for chunk in pd.read_csv(file_name, chunksize=chunksize):\n",
    "    \n",
    "    with open('river_samples.txt', 'w', encoding='utf-8') as f:\n",
    "\n",
    "        for index, row in chunk.iterrows():\n",
    "            x = ast.literal_eval(row['passage'])\n",
    "            text = ' '.join(word for word in x)\n",
    "            f.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing to find bad articles in initial runs of the app-template/tree/master/app\n",
    "import ast\n",
    "import pandas as pd\n",
    "file_name = 'dam_year_22h43m_07Dec2018_603d272/dam-cand-df.csv'\n",
    "chunksize = 10 ** 6\n",
    "\n",
    "\n",
    "for chunk in pd.read_csv(file_name, chunksize=chunksize):\n",
    "    \n",
    "    for index, row in chunk.iterrows():\n",
    "        #if '54fe8e9be138239936c6eabe' in row['docid']:\n",
    "        #    print (row['passage'])\n",
    "        #Checks for docs with text str\n",
    "        x = ast.literal_eval(row['passage'])\n",
    "        for word in x:\n",
    "            if word == 'visantiPooornmteanabtloliyavlrefeitfeheledrrdiesaddtuteomfiansaenudpdblpyif':\n",
    "                print (row['docid'])\n",
    "                print (row['sentid'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!open ."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
